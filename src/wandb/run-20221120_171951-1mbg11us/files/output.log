
  0%|                                                             | 0/160 [00:00<?, ?it/s]/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "/home/nhtlong/playground/zalo-ai/applybigdata/shopee-reviews-sentiment-analysis/src/train.py", line 61, in <module>
    trainer.train()
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py", line 1498, in train
    return inner_training_loop(
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py", line 1740, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py", line 2488, in training_step
    loss.backward()
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 143, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 96, in reduce_add
    nccl.reduce(inputs, output=result, root=root_index)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/nccl.py", line 72, in reduce
    _check_sequence_type(inputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/nccl.py", line 51, in _check_sequence_type
    if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):
AttributeError: module 'collections' has no attribute 'Container'
[31m╭────────────────────────── [39m[1mTraceback (most recent call last)[31m[22m ───────────────────────────╮
[31m│[39m /home/nhtlong/playground/zalo-ai/applybigdata/shopee-reviews-sentiment-analysis/src/[1mtr[22m [31m│
[31m│[39m [1main.py[22m:[94m61[39m in [92m<module>[39m                                                                  [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   58 │   compute_metrics=compute_metrics,                                              [31m│
[31m│[39m   59 )                                                                                 [31m│
[31m│[39m   60                                                                                   [31m│
[31m│[39m [31m❱ [39m61 trainer.train()                                                                   [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/[1mtrainer.py[22m: [31m│
[31m│[39m [94m1498[39m in [92mtrain[39m                                                                          [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   1495 │   │   inner_training_loop = find_executable_batch_size(                       [31m│
[31m│[39m   1496 │   │   │   [96mself[39m._inner_training_loop, [96mself[39m._train_batch_size, args.auto_find_b [31m│
[31m│[39m   1497 │   │   )                                                                       [31m│
[31m│[39m [31m❱ [39m1498 │   │   [94mreturn[39m inner_training_loop(                                             [31m│
[31m│[39m   1499 │   │   │   args=args,                                                          [31m│
[31m│[39m   1500 │   │   │   resume_from_checkpoint=resume_from_checkpoint,                      [31m│
[31m│[39m   1501 │   │   │   trial=trial,                                                        [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/[1mtrainer.py[22m: [31m│
[31m│[39m [94m1740[39m in [92m_inner_training_loop[39m                                                           [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   1737 │   │   │   │   │   [94mwith[39m model.no_sync():                                       [31m│
[31m│[39m   1738 │   │   │   │   │   │   tr_loss_step = [96mself[39m.training_step(model, inputs)        [31m│
[31m│[39m   1739 │   │   │   │   [94melse[39m:                                                           [31m│
[31m│[39m [31m❱ [39m1740 │   │   │   │   │   tr_loss_step = [96mself[39m.training_step(model, inputs)            [31m│
[31m│[39m   1741 │   │   │   │                                                                   [31m│
[31m│[39m   1742 │   │   │   │   [94mif[39m (                                                            [31m│
[31m│[39m   1743 │   │   │   │   │   args.logging_nan_inf_filter                                 [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/[1mtrainer.py[22m: [31m│
[31m│[39m [94m2488[39m in [92mtraining_step[39m                                                                  [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   2485 │   │   │   # loss gets scaled under gradient_accumulation_steps in deepspeed   [31m│
[31m│[39m   2486 │   │   │   loss = [96mself[39m.deepspeed.backward(loss)                                [31m│
[31m│[39m   2487 │   │   [94melse[39m:                                                                   [31m│
[31m│[39m [31m❱ [39m2488 │   │   │   loss.backward()                                                     [31m│
[31m│[39m   2489 │   │                                                                           [31m│
[31m│[39m   2490 │   │   [94mreturn[39m loss.detach()                                                    [31m│
[31m│[39m   2491                                                                                 [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/[1m_tensor.py[22m:[94m363[39m in  [31m│
[31m│[39m [92mbackward[39m                                                                               [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m    360 │   │   │   │   retain_graph=retain_graph,                                      [31m│
[31m│[39m    361 │   │   │   │   create_graph=create_graph,                                      [31m│
[31m│[39m    362 │   │   │   │   inputs=inputs)                                                  [31m│
[31m│[39m [31m❱ [39m 363 │   │   torch.autograd.backward([96mself[39m, gradient, retain_graph, create_graph, inp [31m│
[31m│[39m    364 │                                                                               [31m│
[31m│[39m    365 │   [94mdef[39m [92mregister_hook[39m([96mself[39m, hook):                                              [31m│
[31m│[39m    366 │   │   [33mr"""Registers a backward hook.[39m                                          [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/[1m__init__.[22m [31m│
[31m│[39m [1mpy[22m:[94m173[39m in [92mbackward[39m                                                                     [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   170 │   # The reason we repeat same the comment below is that                        [31m│
[31m│[39m   171 │   # some Python versions print out the first line of a multi-line function     [31m│
[31m│[39m   172 │   # calls in the traceback and some print out the last line                    [31m│
[31m│[39m [31m❱ [39m173 │   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run [31m│
[31m│[39m   174 │   │   tensors, grad_tensors_, retain_graph, create_graph, inputs,              [31m│
[31m│[39m   175 │   │   allow_unreachable=[94mTrue[39m, accumulate_grad=[94mTrue[39m)  # Calls into the C++ engi [31m│
[31m│[39m   176                                                                                  [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/[1mfunction.[22m [31m│
[31m│[39m [1mpy[22m:[94m253[39m in [92mapply[39m                                                                        [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   250 │   │   │   │   │   │   │      [33m"Function is not allowed. You should only impleme[39m [31m│
[31m│[39m   251 │   │   │   │   │   │   │      [33m"of them."[39m)                                       [31m│
[31m│[39m   252 │   │   user_fn = vjp_fn [94mif[39m vjp_fn [95mis[39m [95mnot[39m Function.vjp [94melse[39m backward_fn          [31m│
[31m│[39m [31m❱ [39m253 │   │   [94mreturn[39m user_fn([96mself[39m, *args)                                              [31m│
[31m│[39m   254 │                                                                                [31m│
[31m│[39m   255 │   [94mdef[39m [92mapply_jvp[39m([96mself[39m, *args):                                                  [31m│
[31m│[39m   256 │   │   # _forward_cls is defined by derived class                               [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1m_funct[22m [31m│
[31m│[39m [1mions.py[22m:[94m34[39m in [92mbackward[39m                                                                 [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m    31 │                                                                                [31m│
[31m│[39m    32 │   [1m@staticmethod[22m                                                                [31m│
[31m│[39m    33 │   [94mdef[39m [92mbackward[39m(ctx, *grad_outputs):                                            [31m│
[31m│[39m [31m❱ [39m 34 │   │   [94mreturn[39m ([94mNone[39m,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inpu [31m│
[31m│[39m    35                                                                                  [31m│
[31m│[39m    36                                                                                  [31m│
[31m│[39m    37 [94mclass[39m [4mReduceAddCoalesced[24m(Function):                                              [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1m_funct[22m [31m│
[31m│[39m [1mions.py[22m:[94m45[39m in [92mforward[39m                                                                  [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m    42 │   │                                                                            [31m│
[31m│[39m    43 │   │   grads_ = [grads[i:i + num_inputs]                                        [31m│
[31m│[39m    44 │   │   │   │     [94mfor[39m i [95min[39m [96mrange[39m([94m0[39m, [96mlen[39m(grads), num_inputs)]                     [31m│
[31m│[39m [31m❱ [39m 45 │   │   [94mreturn[39m comm.reduce_add_coalesced(grads_, destination)                    [31m│
[31m│[39m    46 │                                                                                [31m│
[31m│[39m    47 │   [1m@staticmethod[22m                                                                [31m│
[31m│[39m    48 │   [94mdef[39m [92mbackward[39m(ctx, *grad_outputs):                                            [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1mcomm.p[22m [31m│
[31m│[39m [1my[22m:[94m143[39m in [92mreduce_add_coalesced[39m                                                          [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m   140 │   # now the dense ones, which have consistent sizes                            [31m│
[31m│[39m   141 │   [94mfor[39m chunks [95min[39m [96mzip[39m(*itrs):                                                    [31m│
[31m│[39m   142 │   │   flat_tensors = [_flatten_dense_tensors(chunk) [94mfor[39m chunk [95min[39m chunks]  # (n [31m│
[31m│[39m [31m❱ [39m143 │   │   flat_result = reduce_add(flat_tensors, destination)                      [31m│
[31m│[39m   144 │   │   [94mfor[39m t [95min[39m _unflatten_dense_tensors(flat_result, chunks[[94m0[39m]):               [31m│
[31m│[39m   145 │   │   │   # The unflattened tensors do not share storage, and we don't expose  [31m│
[31m│[39m   146 │   │   │   # base flat tensor anyways, so give them different version counters. [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1mcomm.p[22m [31m│
[31m│[39m [1my[22m:[94m96[39m in [92mreduce_add[39m                                                                     [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m    93 │                                                                                [31m│
[31m│[39m    94 │   [94mif[39m nccl.is_available(inputs):                                                [31m│
[31m│[39m    95 │   │   result = torch.empty_like(inputs[root_index])                            [31m│
[31m│[39m [31m❱ [39m 96 │   │   nccl.reduce(inputs, output=result, root=root_index)                      [31m│
[31m│[39m    97 │   [94melse[39m:                                                                        [31m│
[31m│[39m    98 │   │   destination_device = torch.device(inputs[root_index].device.type, destin [31m│
[31m│[39m    99 │   │   nonroot = [t [94mfor[39m i, t [95min[39m [96menumerate[39m(inputs) [94mif[39m i != root_index]           [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/[1mnccl.py[22m:[94m72[39m in [31m│
[31m│[39m [92mreduce[39m                                                                                 [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m    69 │   │      streams: Optional[Sequence[torch.cuda.Stream]] = [94mNone[39m,                [31m│
[31m│[39m    70 │   │      comms=[94mNone[39m, *,                                                        [31m│
[31m│[39m    71 │   │      outputs: Optional[Sequence[torch.Tensor]] = [94mNone[39m) -> [94mNone[39m:            [31m│
[31m│[39m [31m❱ [39m 72 │   _check_sequence_type(inputs)                                                 [31m│
[31m│[39m    73 │   _output: torch.Tensor                                                        [31m│
[31m│[39m    74 │   [94mif[39m outputs [95mis[39m [95mnot[39m [94mNone[39m:                                                      [31m│
[31m│[39m    75 │   │   [94mif[39m output [95mis[39m [95mnot[39m [94mNone[39m:                                                   [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/[1mnccl.py[22m:[94m51[39m in [31m│
[31m│[39m [92m_check_sequence_type[39m                                                                   [31m│
[31m│[39m                                                                                        [31m│
[31m│[39m    48                                                                                  [31m│
[31m│[39m    49                                                                                  [31m│
[31m│[39m    50 [94mdef[39m [92m_check_sequence_type[39m(inputs: Union[torch.Tensor, Sequence[torch.Tensor]]) -> [31m│
[31m│[39m [31m❱ [39m 51 │   [94mif[39m [95mnot[39m [96misinstance[39m(inputs, collections.Container) [95mor[39m [96misinstance[39m(inputs, torch [31m│
[31m│[39m    52 │   │   [94mraise[39m [96mTypeError[39m([33m"Inputs should be a collection of tensors"[39m)              [31m│
[31m│[39m    53                                                                                  [31m│
[31m│[39m    54                                                                                  [31m│
[31m╰────────────────────────────────────────────────────────────────────────────────────────╯
[1mAttributeError: [22mmodule [32m'collections'[39m has no attribute [32m'Container'