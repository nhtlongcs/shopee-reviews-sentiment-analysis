
  0%|                                                             | 0/160 [00:00<?, ?it/s]/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
Traceback (most recent call last):
  File "/home/nhtlong/playground/zalo-ai/applybigdata/shopee-reviews-sentiment-analysis/src/train.py", line 61, in <module>
    trainer.train()
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py", line 1498, in train
    return inner_training_loop(
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py", line 1740, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/trainer.py", line 2488, in training_step
    loss.backward()
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/_tensor.py", line 363, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/__init__.py", line 173, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/function.py", line 253, in apply
    return user_fn(self, *args)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 34, in backward
    return (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/_functions.py", line 45, in forward
    return comm.reduce_add_coalesced(grads_, destination)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 143, in reduce_add_coalesced
    flat_result = reduce_add(flat_tensors, destination)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/comm.py", line 96, in reduce_add
    nccl.reduce(inputs, output=result, root=root_index)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/nccl.py", line 72, in reduce
    _check_sequence_type(inputs)
  File "/home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/nccl.py", line 51, in _check_sequence_type
    if not isinstance(inputs, collections.Container) or isinstance(inputs, torch.Tensor):
AttributeError: module 'collections' has no attribute 'Container'
[31mâ•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ [39m[1mTraceback (most recent call last)[31m[22m â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
[31mâ”‚[39m /home/nhtlong/playground/zalo-ai/applybigdata/shopee-reviews-sentiment-analysis/src/[1mtr[22m [31mâ”‚
[31mâ”‚[39m [1main.py[22m:[94m61[39m in [92m<module>[39m                                                                  [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   58 â”‚   compute_metrics=compute_metrics,                                              [31mâ”‚
[31mâ”‚[39m   59 )                                                                                 [31mâ”‚
[31mâ”‚[39m   60                                                                                   [31mâ”‚
[31mâ”‚[39m [31mâ± [39m61 trainer.train()                                                                   [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/[1mtrainer.py[22m: [31mâ”‚
[31mâ”‚[39m [94m1498[39m in [92mtrain[39m                                                                          [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   1495 â”‚   â”‚   inner_training_loop = find_executable_batch_size(                       [31mâ”‚
[31mâ”‚[39m   1496 â”‚   â”‚   â”‚   [96mself[39m._inner_training_loop, [96mself[39m._train_batch_size, args.auto_find_b [31mâ”‚
[31mâ”‚[39m   1497 â”‚   â”‚   )                                                                       [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1498 â”‚   â”‚   [94mreturn[39m inner_training_loop(                                             [31mâ”‚
[31mâ”‚[39m   1499 â”‚   â”‚   â”‚   args=args,                                                          [31mâ”‚
[31mâ”‚[39m   1500 â”‚   â”‚   â”‚   resume_from_checkpoint=resume_from_checkpoint,                      [31mâ”‚
[31mâ”‚[39m   1501 â”‚   â”‚   â”‚   trial=trial,                                                        [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/[1mtrainer.py[22m: [31mâ”‚
[31mâ”‚[39m [94m1740[39m in [92m_inner_training_loop[39m                                                           [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   1737 â”‚   â”‚   â”‚   â”‚   â”‚   [94mwith[39m model.no_sync():                                       [31mâ”‚
[31mâ”‚[39m   1738 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   tr_loss_step = [96mself[39m.training_step(model, inputs)        [31mâ”‚
[31mâ”‚[39m   1739 â”‚   â”‚   â”‚   â”‚   [94melse[39m:                                                           [31mâ”‚
[31mâ”‚[39m [31mâ± [39m1740 â”‚   â”‚   â”‚   â”‚   â”‚   tr_loss_step = [96mself[39m.training_step(model, inputs)            [31mâ”‚
[31mâ”‚[39m   1741 â”‚   â”‚   â”‚   â”‚                                                                   [31mâ”‚
[31mâ”‚[39m   1742 â”‚   â”‚   â”‚   â”‚   [94mif[39m (                                                            [31mâ”‚
[31mâ”‚[39m   1743 â”‚   â”‚   â”‚   â”‚   â”‚   args.logging_nan_inf_filter                                 [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/transformers/[1mtrainer.py[22m: [31mâ”‚
[31mâ”‚[39m [94m2488[39m in [92mtraining_step[39m                                                                  [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   2485 â”‚   â”‚   â”‚   # loss gets scaled under gradient_accumulation_steps in deepspeed   [31mâ”‚
[31mâ”‚[39m   2486 â”‚   â”‚   â”‚   loss = [96mself[39m.deepspeed.backward(loss)                                [31mâ”‚
[31mâ”‚[39m   2487 â”‚   â”‚   [94melse[39m:                                                                   [31mâ”‚
[31mâ”‚[39m [31mâ± [39m2488 â”‚   â”‚   â”‚   loss.backward()                                                     [31mâ”‚
[31mâ”‚[39m   2489 â”‚   â”‚                                                                           [31mâ”‚
[31mâ”‚[39m   2490 â”‚   â”‚   [94mreturn[39m loss.detach()                                                    [31mâ”‚
[31mâ”‚[39m   2491                                                                                 [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/[1m_tensor.py[22m:[94m363[39m in  [31mâ”‚
[31mâ”‚[39m [92mbackward[39m                                                                               [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m    360 â”‚   â”‚   â”‚   â”‚   retain_graph=retain_graph,                                      [31mâ”‚
[31mâ”‚[39m    361 â”‚   â”‚   â”‚   â”‚   create_graph=create_graph,                                      [31mâ”‚
[31mâ”‚[39m    362 â”‚   â”‚   â”‚   â”‚   inputs=inputs)                                                  [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 363 â”‚   â”‚   torch.autograd.backward([96mself[39m, gradient, retain_graph, create_graph, inp [31mâ”‚
[31mâ”‚[39m    364 â”‚                                                                               [31mâ”‚
[31mâ”‚[39m    365 â”‚   [94mdef[39m [92mregister_hook[39m([96mself[39m, hook):                                              [31mâ”‚
[31mâ”‚[39m    366 â”‚   â”‚   [33mr"""Registers a backward hook.[39m                                          [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/[1m__init__.[22m [31mâ”‚
[31mâ”‚[39m [1mpy[22m:[94m173[39m in [92mbackward[39m                                                                     [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   170 â”‚   # The reason we repeat same the comment below is that                        [31mâ”‚
[31mâ”‚[39m   171 â”‚   # some Python versions print out the first line of a multi-line function     [31mâ”‚
[31mâ”‚[39m   172 â”‚   # calls in the traceback and some print out the last line                    [31mâ”‚
[31mâ”‚[39m [31mâ± [39m173 â”‚   Variable._execution_engine.run_backward(  # Calls into the C++ engine to run [31mâ”‚
[31mâ”‚[39m   174 â”‚   â”‚   tensors, grad_tensors_, retain_graph, create_graph, inputs,              [31mâ”‚
[31mâ”‚[39m   175 â”‚   â”‚   allow_unreachable=[94mTrue[39m, accumulate_grad=[94mTrue[39m)  # Calls into the C++ engi [31mâ”‚
[31mâ”‚[39m   176                                                                                  [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/autograd/[1mfunction.[22m [31mâ”‚
[31mâ”‚[39m [1mpy[22m:[94m253[39m in [92mapply[39m                                                                        [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   250 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      [33m"Function is not allowed. You should only impleme[39m [31mâ”‚
[31mâ”‚[39m   251 â”‚   â”‚   â”‚   â”‚   â”‚   â”‚   â”‚      [33m"of them."[39m)                                       [31mâ”‚
[31mâ”‚[39m   252 â”‚   â”‚   user_fn = vjp_fn [94mif[39m vjp_fn [95mis[39m [95mnot[39m Function.vjp [94melse[39m backward_fn          [31mâ”‚
[31mâ”‚[39m [31mâ± [39m253 â”‚   â”‚   [94mreturn[39m user_fn([96mself[39m, *args)                                              [31mâ”‚
[31mâ”‚[39m   254 â”‚                                                                                [31mâ”‚
[31mâ”‚[39m   255 â”‚   [94mdef[39m [92mapply_jvp[39m([96mself[39m, *args):                                                  [31mâ”‚
[31mâ”‚[39m   256 â”‚   â”‚   # _forward_cls is defined by derived class                               [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1m_funct[22m [31mâ”‚
[31mâ”‚[39m [1mions.py[22m:[94m34[39m in [92mbackward[39m                                                                 [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m    31 â”‚                                                                                [31mâ”‚
[31mâ”‚[39m    32 â”‚   [1m@staticmethod[22m                                                                [31mâ”‚
[31mâ”‚[39m    33 â”‚   [94mdef[39m [92mbackward[39m(ctx, *grad_outputs):                                            [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 34 â”‚   â”‚   [94mreturn[39m ([94mNone[39m,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inpu [31mâ”‚
[31mâ”‚[39m    35                                                                                  [31mâ”‚
[31mâ”‚[39m    36                                                                                  [31mâ”‚
[31mâ”‚[39m    37 [94mclass[39m [4mReduceAddCoalesced[24m(Function):                                              [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1m_funct[22m [31mâ”‚
[31mâ”‚[39m [1mions.py[22m:[94m45[39m in [92mforward[39m                                                                  [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m    42 â”‚   â”‚                                                                            [31mâ”‚
[31mâ”‚[39m    43 â”‚   â”‚   grads_ = [grads[i:i + num_inputs]                                        [31mâ”‚
[31mâ”‚[39m    44 â”‚   â”‚   â”‚   â”‚     [94mfor[39m i [95min[39m [96mrange[39m([94m0[39m, [96mlen[39m(grads), num_inputs)]                     [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 45 â”‚   â”‚   [94mreturn[39m comm.reduce_add_coalesced(grads_, destination)                    [31mâ”‚
[31mâ”‚[39m    46 â”‚                                                                                [31mâ”‚
[31mâ”‚[39m    47 â”‚   [1m@staticmethod[22m                                                                [31mâ”‚
[31mâ”‚[39m    48 â”‚   [94mdef[39m [92mbackward[39m(ctx, *grad_outputs):                                            [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1mcomm.p[22m [31mâ”‚
[31mâ”‚[39m [1my[22m:[94m143[39m in [92mreduce_add_coalesced[39m                                                          [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m   140 â”‚   # now the dense ones, which have consistent sizes                            [31mâ”‚
[31mâ”‚[39m   141 â”‚   [94mfor[39m chunks [95min[39m [96mzip[39m(*itrs):                                                    [31mâ”‚
[31mâ”‚[39m   142 â”‚   â”‚   flat_tensors = [_flatten_dense_tensors(chunk) [94mfor[39m chunk [95min[39m chunks]  # (n [31mâ”‚
[31mâ”‚[39m [31mâ± [39m143 â”‚   â”‚   flat_result = reduce_add(flat_tensors, destination)                      [31mâ”‚
[31mâ”‚[39m   144 â”‚   â”‚   [94mfor[39m t [95min[39m _unflatten_dense_tensors(flat_result, chunks[[94m0[39m]):               [31mâ”‚
[31mâ”‚[39m   145 â”‚   â”‚   â”‚   # The unflattened tensors do not share storage, and we don't expose  [31mâ”‚
[31mâ”‚[39m   146 â”‚   â”‚   â”‚   # base flat tensor anyways, so give them different version counters. [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/nn/parallel/[1mcomm.p[22m [31mâ”‚
[31mâ”‚[39m [1my[22m:[94m96[39m in [92mreduce_add[39m                                                                     [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m    93 â”‚                                                                                [31mâ”‚
[31mâ”‚[39m    94 â”‚   [94mif[39m nccl.is_available(inputs):                                                [31mâ”‚
[31mâ”‚[39m    95 â”‚   â”‚   result = torch.empty_like(inputs[root_index])                            [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 96 â”‚   â”‚   nccl.reduce(inputs, output=result, root=root_index)                      [31mâ”‚
[31mâ”‚[39m    97 â”‚   [94melse[39m:                                                                        [31mâ”‚
[31mâ”‚[39m    98 â”‚   â”‚   destination_device = torch.device(inputs[root_index].device.type, destin [31mâ”‚
[31mâ”‚[39m    99 â”‚   â”‚   nonroot = [t [94mfor[39m i, t [95min[39m [96menumerate[39m(inputs) [94mif[39m i != root_index]           [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/[1mnccl.py[22m:[94m72[39m in [31mâ”‚
[31mâ”‚[39m [92mreduce[39m                                                                                 [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m    69 â”‚   â”‚      streams: Optional[Sequence[torch.cuda.Stream]] = [94mNone[39m,                [31mâ”‚
[31mâ”‚[39m    70 â”‚   â”‚      comms=[94mNone[39m, *,                                                        [31mâ”‚
[31mâ”‚[39m    71 â”‚   â”‚      outputs: Optional[Sequence[torch.Tensor]] = [94mNone[39m) -> [94mNone[39m:            [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 72 â”‚   _check_sequence_type(inputs)                                                 [31mâ”‚
[31mâ”‚[39m    73 â”‚   _output: torch.Tensor                                                        [31mâ”‚
[31mâ”‚[39m    74 â”‚   [94mif[39m outputs [95mis[39m [95mnot[39m [94mNone[39m:                                                      [31mâ”‚
[31mâ”‚[39m    75 â”‚   â”‚   [94mif[39m output [95mis[39m [95mnot[39m [94mNone[39m:                                                   [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m /home/nhtlong/.conda/envs/kaggle/lib/python3.10/site-packages/torch/cuda/[1mnccl.py[22m:[94m51[39m in [31mâ”‚
[31mâ”‚[39m [92m_check_sequence_type[39m                                                                   [31mâ”‚
[31mâ”‚[39m                                                                                        [31mâ”‚
[31mâ”‚[39m    48                                                                                  [31mâ”‚
[31mâ”‚[39m    49                                                                                  [31mâ”‚
[31mâ”‚[39m    50 [94mdef[39m [92m_check_sequence_type[39m(inputs: Union[torch.Tensor, Sequence[torch.Tensor]]) -> [31mâ”‚
[31mâ”‚[39m [31mâ± [39m 51 â”‚   [94mif[39m [95mnot[39m [96misinstance[39m(inputs, collections.Container) [95mor[39m [96misinstance[39m(inputs, torch [31mâ”‚
[31mâ”‚[39m    52 â”‚   â”‚   [94mraise[39m [96mTypeError[39m([33m"Inputs should be a collection of tensors"[39m)              [31mâ”‚
[31mâ”‚[39m    53                                                                                  [31mâ”‚
[31mâ”‚[39m    54                                                                                  [31mâ”‚
[31mâ•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
[1mAttributeError: [22mmodule [32m'collections'[39m has no attribute [32m'Container'