
  0%|                                                             | 0/160 [00:00<?, ?it/s]/home/nhtlong/miniconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '




 20%|██████████▍                                         | 32/160 [00:11<00:30,  4.16it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 32

 97%|███████████████████████████████████████████████████▎ | 31/32 [00:03<00:00,  9.42it/s]
  warnings.warn('Was asked to gather along dimension 0, but all '
 24%|████████████▎                                       | 38/160 [00:25<01:47,  1.13it/s]



 40%|████████████████████▊                               | 64/160 [00:32<00:23,  4.16it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 32


 97%|███████████████████████████████████████████████████▎ | 31/32 [00:03<00:00,  9.40it/s]
  warnings.warn('Was asked to gather along dimension 0, but all '



 60%|███████████████████████████████▏                    | 96/160 [00:52<00:15,  4.17it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 32

 97%|███████████████████████████████████████████████████▎ | 31/32 [00:03<00:00,  9.19it/s]
  warnings.warn('Was asked to gather along dimension 0, but all '



 80%|████████████████████████████████████████▊          | 128/160 [01:12<00:07,  4.16it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 32


  warnings.warn('Was asked to gather along dimension 0, but all '
{'eval_loss': 0.1755332201719284, 'eval_accuracy': 0.944, 'eval_f1': 0.9549839228295819, 'eval_precision': 0.9209302325581395, 'eval_recall': 0.991652754590985, 'eval_runtime': 11.3428, 'eval_samples_per_second': 88.161, 'eval_steps_per_second': 2.821, 'epoch': 4.0}



100%|███████████████████████████████████████████████████| 160/160 [01:32<00:00,  4.16it/s]The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
***** Running Evaluation *****
  Num examples = 1000
  Batch size = 32

 97%|███████████████████████████████████████████████████▎ | 31/32 [00:03<00:00,  9.37it/s]
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████| 160/160 [01:43<00:00,  1.54it/s]
{'eval_loss': 0.13187609612941742, 'eval_accuracy': 0.955, 'eval_f1': 0.9630844954881049, 'eval_precision': 0.9467741935483871, 'eval_recall': 0.9799666110183639, 'eval_runtime': 11.3629, 'eval_samples_per_second': 88.005, 'eval_steps_per_second': 2.816, 'epoch': 5.0}
{'train_runtime': 111.8469, 'train_samples_per_second': 44.704, 'train_steps_per_second': 1.431, 'train_loss': 0.1674456238746643, 'epoch': 5.0}